{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95559aef-383d-4e6c-a4c9-5a760f1ab6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dfd4645d-2c5c-4505-864c-c8c77919f582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data read \n",
    "words = open('/home/risal/X3s4c5/Makemore/names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c03b3cee-1fcd-45a7-ab69-4efcbf7923e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "298ce7de-3e75-4150-884f-f6dcdfc4d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "def build_dataset(words):\n",
    "    X, Y= [] , []\n",
    "    \n",
    "    for w in words:\n",
    "        context  = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X,Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_trn, Y_trn = build_dataset(words[:n1])\n",
    "X_dev, Y_dev = build_dataset(words[n1:n2])\n",
    "X_tst, Y_tst = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2424fcdc-6d36-4638-85d6-c59d0776cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining utility fucntion, used to compare manual backpropagation\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "57b0530a-10d2-471d-bc04-d85f24c29720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 200\n",
    "\n",
    "# MLP components \n",
    "g = torch.Generator().manual_seed(123)\n",
    "C = torch.randn((vocab_size,n_emb), generator = g)\n",
    "# Linear 1\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden), generator = g) * (5/3)/((n_emb * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g)  * 0\n",
    "# Linear 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g ) * 0\n",
    "# batchNorm parameters \n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d872d61-687d-4213-b255-47f120c70b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n= batch_size\n",
    "#  Construct mini batch\n",
    "ix = torch.randint(0,X_trn.shape[0], (batch_size,), generator = g)\n",
    "Xb, Yb = X_trn[ix], Y_trn[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b7f0726c-8b43-494c-bbc5-91726b2c736f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7188, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0],-1)\n",
    "# linear layer 1\n",
    "hprebn = embcat @ W1 + b1 \n",
    "# BatchNorm layer\n",
    "bnmeani = 1 /n * hprebn.sum(0, keepdim =True) #hprebn.mean(0, keepdim = True) \n",
    "bndiff = hprebn - bnmeani    #(Xi - mean)\n",
    "bndiff2 = bndiff**2          \n",
    "bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim = True)  #Bessel's correction sampled by (n-1) \n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity \n",
    "h = torch.tanh(hpreact)\n",
    "# linear layer 2 \n",
    "logits = h @ W2 + b2\n",
    "# Loss calculation(calculate loglikelihood)\n",
    "logit_maxes = logits.max(1, keepdim = True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim = True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# Pytorch backward pass \n",
    "for p in parameters: \n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts_sum_inv, counts_sum, counts, norm_logits, logit_maxes, logits, h, hpreact,\n",
    "         bnraw, bnvar_inv, bnvar, bndiff2, bndiff, bnmeani,hprebn, embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9c0dd-7950-47be-854f-e494649fed28",
   "metadata": {},
   "source": [
    "# Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25dcd589-c726-4244-948d-231a50e3caff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = - 1.0/ n \n",
    "dprobs = (1 / probs) * dlogprobs # Using chain rule (local derivative times the derivative of loss with respective to its output)\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim =True)  # ?? (managing broadcasting of [32,27] to [32,1])\n",
    "dcounts = counts_sum_inv * dprobs \n",
    "dcounts_sum = (-1*counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum  #\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogit_maxes = (-1 * dnorm_logits).sum(1, keepdim = True)\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes = logits.shape[1]) * dlogit_maxes\n",
    "# Linear layer 2\n",
    "dh = dlogits @ W2.T\n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0) \n",
    "# Non-linearity\n",
    "dhpreact = (1.0 - h**2) * dh    ### minor error \n",
    "# BatchNorm layer \n",
    "dbnbias = dhpreact.sum(0, keepdim = True)\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbndiff = bnvar_inv * dbnraw  ## has multiple branches \n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim = True)\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\n",
    "dbndiff2 = (1 / (n-1) * torch.ones_like(bndiff2)) * dbnvar\n",
    "dbndiff += (2 * bndiff) * dbndiff2 # another branch of dbndiff \n",
    "dbnmeani = (-1 * dbndiff).sum(0, keepdim = True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += (1 /n * torch.ones_like(hprebn)) * dbnmeani \n",
    "# Linear layer 1\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k, j]\n",
    "        dC[ix] += demb[k,j ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61f1d3-bdc9-45cb-bb7b-3062e658efdd",
   "metadata": {},
   "source": [
    "# Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98d02dd5-fe03-46d9-a93e-1bb748196e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogit_maxes    | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dlogits         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dh              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dW2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "db2             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "dhpreact        | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbnbias         | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n",
      "dbngain         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "dbnraw          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "dbndiff         | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "dbnvar_inv      | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
      "dbnvar          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dbndiff2        | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
      "dbnmeani        | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dhprebn         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
      "dembcat         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dW1             | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
      "db1             | exact: False | approximate: True  | maxdiff: 4.423782229423523e-09\n",
      "demb            | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "dC              | exact: False | approximate: True  | maxdiff: 1.4901161193847656e-08\n"
     ]
    }
   ],
   "source": [
    "cmp('logprobs', dlogprobs, logprobs )\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
    "cmp('dlogit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('dlogits', dlogits, logits)\n",
    "cmp('dh', dh, h)\n",
    "cmp('dW2', dW2, W2)\n",
    "cmp('db2', db2, b2)\n",
    "cmp('dhpreact', dhpreact, hpreact)\n",
    "cmp('dbnbias',  dbnbias, bnbias)\n",
    "cmp('dbngain', dbngain, bngain)\n",
    "cmp('dbnraw', dbnraw, bnraw)\n",
    "cmp('dbndiff', dbndiff, bndiff)\n",
    "cmp('dbnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('dbnvar',dbnvar, bnvar )\n",
    "cmp('dbndiff2', dbndiff2 , bndiff2)\n",
    "cmp('dbnmeani', dbnmeani, bnmeani)\n",
    "cmp('dhprebn', dhprebn, hprebn)\n",
    "cmp('dembcat', dembcat, embcat)\n",
    "cmp('dW1', dW1, W1)\n",
    "cmp('db1', db1, b1)\n",
    "cmp('demb', demb , emb)\n",
    "cmp('dC', dC, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e197ca4-727e-40e7-9998-1ca3d70c2ce6",
   "metadata": {},
   "source": [
    "# Backprop through actual chunk of mathmetical operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bbccca-9d46-415b-98e4-5a9d3f21b7a4",
   "metadata": {},
   "source": [
    "## Through losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b62ecdff-b21b-4f6a-9e80-bff1be69d67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7187986373901367 diff: -2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:',(loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74f178df-aa49-407d-9711-99b44b1dfee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlogits         | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
     ]
    }
   ],
   "source": [
    "# Backward pass \n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n),Yb] -= 1\n",
    "dlogits /= n\n",
    "cmp('dlogits', dlogits, logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dad87-0164-45e2-99cd-c500545c753e",
   "metadata": {},
   "source": [
    "## Through BatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "af1da6dc-de01-45b5-a16f-2cc304d05205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_diff: tensor(2.2411e-05, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass \n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim = True)) / torch.sqrt(hprebn.var(0, keepdim =True, unbiased=True))\n",
    "print('max_diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74cf470e-9147-439f-850c-1a758a1c5331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dhprebn         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# Calculate hprebn given dhpreact; might need some variable from the forward pass\n",
    "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "cmp('dhprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3c47a-ee7e-4ca8-8e21-7ffdfdc26c67",
   "metadata": {},
   "source": [
    "# Putting all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aacff47b-89da-4860-a1e6-6a3569cdb29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 200\n",
    "\n",
    "# MLP components \n",
    "g = torch.Generator().manual_seed(123)\n",
    "C = torch.randn((vocab_size,n_emb), generator = g)\n",
    "# Linear 1\n",
    "W1 = torch.randn((n_emb * block_size, n_hidden), generator = g) * (5/3)/((n_emb * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator = g)  * 0\n",
    "# Linear 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator = g ) * 0\n",
    "# batchNorm parameters \n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6107fea4-cca3-455e-9bf3-14d74832275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0 /   30000: 3.7188\n",
      "   1000 /   30000: 2.2761\n",
      "   2000 /   30000: 2.4004\n",
      "   3000 /   30000: 2.4984\n",
      "   4000 /   30000: 2.2252\n",
      "   5000 /   30000: 2.0395\n",
      "   6000 /   30000: 2.1538\n",
      "   7000 /   30000: 2.2782\n",
      "   8000 /   30000: 2.3779\n",
      "   9000 /   30000: 2.3555\n",
      "  10000 /   30000: 2.1661\n",
      "  11000 /   30000: 2.5271\n",
      "  12000 /   30000: 2.7327\n",
      "  13000 /   30000: 2.4283\n",
      "  14000 /   30000: 2.2728\n",
      "  15000 /   30000: 1.9629\n",
      "  16000 /   30000: 2.6917\n",
      "  17000 /   30000: 2.3592\n",
      "  18000 /   30000: 2.0944\n",
      "  19000 /   30000: 2.3980\n",
      "  20000 /   30000: 2.0161\n",
      "  21000 /   30000: 2.1292\n",
      "  22000 /   30000: 2.6313\n",
      "  23000 /   30000: 2.0480\n",
      "  24000 /   30000: 2.8023\n",
      "  25000 /   30000: 1.9816\n",
      "  26000 /   30000: 2.5508\n",
      "  27000 /   30000: 2.2111\n",
      "  28000 /   30000: 1.8077\n",
      "  29000 /   30000: 2.1178\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "max_steps = 30000\n",
    "batch_size = 32\n",
    "losses_train = []\n",
    "with torch.no_grad():\n",
    "    for i in range(max_steps):\n",
    "        \n",
    "        # mini batch\n",
    "        ix = torch.randint(0,X_trn.shape[0], (batch_size,), generator = g)\n",
    "        Xb, Yb = X_trn[ix], Y_trn[ix]\n",
    "        \n",
    "        # forward passes \n",
    "        emb = C[Xb]\n",
    "        embcat = emb.view(emb.shape[0],-1)\n",
    "        # linear layer 1\n",
    "        hprebn = embcat @ W1 + b1 \n",
    "        # BatchNorm layer\n",
    "        bnmeani = 1 /n * hprebn.sum(0, keepdim =True) #hprebn.mean(0, keepdim = True) \n",
    "        bndiff = hprebn - bnmeani    #(Xi - mean)\n",
    "        bndiff2 = bndiff**2          \n",
    "        bnvar = 1 / (n-1) * (bndiff2).sum(0, keepdim = True)  #Bessel's correction sampled by (n-1) \n",
    "        bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "        bnraw = bndiff * bnvar_inv\n",
    "        hpreact = bngain * bnraw + bnbias\n",
    "        # Non-linearity \n",
    "        h = torch.tanh(hpreact)\n",
    "        # linear layer 2 \n",
    "        logits = h @ W2 + b2\n",
    "        loss = F.cross_entropy(logits,  Y_trn[ix]) \n",
    "    \n",
    "        # backward pass \n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        # loss.backward()\n",
    "        ## Code manual backpropagation here\n",
    "        # Loss \n",
    "        dlogits = F.softmax(logits, 1)\n",
    "        dlogits[range(n),Yb] -= 1\n",
    "        dlogits /= n\n",
    "        # Linear layer 2\n",
    "        dh = dlogits @ W2.T\n",
    "        dW2 = h.T @ dlogits\n",
    "        db2 = dlogits.sum(0) \n",
    "                \n",
    "        # Non-linearity\n",
    "        dhpreact = (1.0 - h**2) * dh    \n",
    "        # BatchNorm layer\n",
    "        dbnbias = dhpreact.sum(0, keepdim = True)\n",
    "        dbngain = (bnraw * dhpreact).sum(0, keepdim = True)\n",
    "        dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "            \n",
    "        # Linear layer 1\n",
    "        dembcat = dhprebn @ W1.T\n",
    "        dW1 = embcat.T @ dhprebn\n",
    "        db1 = dhprebn.sum(0)\n",
    "        demb = dembcat.view(emb.shape)\n",
    "        dC = torch.zeros_like(C)\n",
    "        for k in range(Xb.shape[0]):\n",
    "            for j in range(Xb.shape[1]):\n",
    "                ix = Xb[k, j]\n",
    "                dC[ix] += demb[k,j ] \n",
    "        ### parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "        # dC, dW1, db1, dW2, db2, dbngain, dbnbias = None, None,None, None, None, None,None\n",
    "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "        # update params\n",
    "        lr = 0.1 \n",
    "        for p,grad in zip(parameters, grads):\n",
    "            # p.data += -lr * p.grad\n",
    "            p.data += -lr * grad\n",
    "\n",
    "        # track stats\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"{i:7d} / {max_steps:7d}: {loss.item():.4f}\")\n",
    "        losses_train.append(loss.item())\n",
    "\n",
    "    # if i >= 100:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3441eb51-bdfd-4ba6-8b87-e82b365d9927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking grad values \n",
    "# for p, g in  zip(parameters, grads):\n",
    "#     cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3626a245-b472-4c87-8097-b0c85ef791f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "with torch.no_grad():\n",
    "    emb = C[X_trn]\n",
    "    embcat = emb.view((emb.shape[0], -1))\n",
    "    hpreact = embcat@ W1 + b1\n",
    "    bnmean = hpreact.mean(0, keepdim = True)\n",
    "    bnvar = hpreact.var(0, keepdim = True, unbiased = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b596df4c-c8fa-409d-adb4-1555803be836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.1506688594818115\n",
      "val 2.4073147773742676\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    X,Y = {\n",
    "        'train': (X_trn, Y_trn),\n",
    "        'val' : (X_dev, Y_dev),\n",
    "        'test' : (X_tst, Y_tst),\n",
    "    }[split]\n",
    "    emb = C[X]\n",
    "    embcat = emb.view((emb.shape[0], -1))\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)** -0.5 + bnbias # Batch norm formula and scaling and shifting  \n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits,  Y) \n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ce663b2b-3325-4632-8860-e50b5d5b77f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agjznjjzzhvtjqwmwkzjrjgjjfuygfgjzjwjhrjdbazaqwfwfjlffzdguzkmhgqzzggkggehpgdgfkgggbgujghjqfzwcjujjjzgjgjzkccgjqqsbzjkgjzkjtjgjbcgkbvcxogwjsfkkjjjhhwjnzhgwjdcjjbgchfcghcjznjazjjrzkgbfjzcjmpggjgeczbyzjggjjjwmkdtglfgjnjzzggbghjghjgjcdjkzfzjkjjbcjymjdsbcwljjgtgdkqhgjyqyjjgzjjtgcjgkjjjgzzjjzcgjrcggffjhgzjgcgqjgggywffkdkjxgyjccfcwcwkajunujgfbbhzjimssbjlcjdshzjccjgyjjqbzgqgdfjgfhjszggcajcwjxjjwrgzjjrjjmrjjujcuzkhzsmjgcalzzmqzmbkjcjzjjpfbzcjzfjgpgzvjzbfbgmggsqjfgsazzjjgjjcaehjqjqkcgjzggfrgigfgjrzhsjzjrmqbjkfkbgrqkkjgjjkznzjgskjsgjugjobkffcffsxqdmfjfgzbbjkgfjzfjqhjqgjcbsjzjkqdagjopzqduzjdwgofjsjmxdwucgbggzdbkkjjglcvgwxkgjjgfffpzgqzjgjjgznfmrugwbfzccjhggzjybgcjhbdcgfgjgbgfdjskzvkrohjggzbgrmggblygjjcjkdjumhhjjjdkjjgrgggnkkxbjfkwdbjfkfzzjwfgxgjwjgjggbgbwvjffgbhgwfjjpbnckfjjfqmsgqjgzfhvffzjjjjfdbhzjgwfkgjjqbsbjcsjjptzjkjgkfgnsgdwntkujgkmbgzcwckzhzjvjznjzhczowgagjhdkgxugqfjcjjfjcjfzrjjhgfgzifjsfqghjjmmajcjbxgjmxzjygajjkcjjjzjjjmhsbgbqmkyzgjggnjdkgjjecjgjjjosgjqgjnjgbjjqfqzkhjggddjbgvggggkfvrghwgigffszbzgzcbgkjjwgcbxgffnwbsdgjjzchjamzrggzgjfssqyqjsgjzsmjqzyjmjffjgcjjwfcjjczgjjfzjgjvgcjgfkjkdkaogbgfjkbcfdgjjjhgjabgzyjkzvgcjhwjmtgjggcfrjzjjjbajuggwjwhzwjjchjgzsjfjgtqzgjjgghjgzsjjhzjjwfzvtjghgjgfgjafrwbgcgjjfnzgjghdszjjfmczbcbzcmfzjjszujfdbgfqjhujgqgjgjhkfxkmgcufycjfkjzgkg.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the model\n",
    "g  = torch.Generator().manual_seed(123)\n",
    "for i in range(1):\n",
    "    out = []\n",
    "    context = [0] * block_size\n",
    "    while True:\n",
    "        emb = C[torch.tensor([context])]\n",
    "        embcat = emb.view((emb.shape[0], -1))\n",
    "        h = torch.tanh(embcat @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        ix = torch.multinomial(probs, num_samples = 1, replacement = True, generator = g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4007b-0f81-4ac5-8657-69a39737c111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
